eval_metric = "rmse",
eta = 0.1,
colsample_bytree = 0.8,
max_depth = 4,
min_child_weight = 1,
nthread = 8,
# base_score = mean(ll),
gamma = 0,
subsample = 0.8
)
watchlist= list(train = dtrain)
set.seed(1235)
fit_cv = xgb.cv(params = param,
data = dtrain,
watchlist = watchlist,
nrounds = 5000,
nfold = 5,
print_every_n = 500,
early_stopping_rounds = 100,
prediction = FALSE,
maximize = FALSE)
set.seed(1235)
mod.xgb = xgb.train(data = dtrain,params = param,nrounds = fit_cv$best_ntreelimit)
imp = as.data.frame(xgb.importance(feature_names = colnames(df_train),model = mod.xgb))
preds= predict(mod.xgb,dtest)
summary(preds)
sub = data.frame(test.id,preds)
colnames(sub) = colnames(samp)
write.csv(sub,file = paste0(subm.dir,"/subm.csv"),row.names = F)
rm(df_train)
dim(df_train)
da = cbind(da,df_train[,272:ncol(df_train)])
de = cbind(de,df_test[,272:ncol(df_train)])
dim(df_test)
de = cbind(lgb = lgbp,lgb2 = predte/5)
de = cbind(de,df_test[,309:ncol(df_train)])
de = cbind(de,df_test[,309:ncol(df_test])
de = cbind(lgb = lgbp,lgb2 = predte/5)
de = cbind(de,df_test[,309:ncol(df_test])
de = cbind(de,df_test[,309:ncol(df_test)])
dtrain = xgb.DMatrix(as.matrix(da), label=label)
dtest = xgb.DMatrix(as.matrix(de))
watchlist= list(train = dtrain)
set.seed(1235)
fit_cv = xgb.cv(params = param,
data = dtrain,
watchlist = watchlist,
nrounds = 5000,
nfold = 5,
print_every_n = 500,
early_stopping_rounds = 100,
prediction = FALSE,
maximize = FALSE)
set.seed(1235)
mod.xgb = xgb.train(data = dtrain,params = param,nrounds = fit_cv$best_ntreelimit)
imp = as.data.frame(xgb.importance(feature_names = colnames(df_train),model = mod.xgb))
preds= predict(mod.xgb,dtest)
#pred = round(pred^2)
sub = data.frame(test.id,preds)
colnames(sub) = colnames(samp)
write.csv(sub,file = paste0(subm.dir,"/subm.csv"),row.names = F)
summary(preds)
#xgboost
df_train = df_train %>% select(-ends_with(c("vap","vpd","B8A","Temp","swe","pdsi","B10","B11")))
df_train$m=NULL
ftrs = data.frame(
type = unlist(lapply(df_train[1:length(train.id),],class)),
n.unique = unlist(lapply(df_train[1:length(train.id),],function(x)length(unique(x)))),
f.missing = unlist(lapply(df_train[1:length(train.id),],function(x)mean(is.na(x)))),
spear.cor = unlist(lapply(df_train[1:length(train.id),],function(x){idx = !is.na(x);
if(is.factor(x)) x = as.numeric(x);
if(is.character(x)) x = as.numeric(as.factor(x));
if(is.integer(x)) x = as.numeric(x);
if(is.logical(x)) x = as.numeric(x);
cor(x[idx],y = label[idx], method = "spearman")
}))
)
ftrs$name = rownames(ftrs)
ftrs = ftrs %>% drop_na()
df_train = df_train[,names(df_train) %in% ftrs$name]
dim(df_train)
lgbp2 = predte/5
devresult = rep(0,nrow(df_train))
predte = rep(0,nrow(df_test))
cvscore = c()
int.seed = c(5000)
for (i in 1:length(int.seed)) {
cat("model training",i,"\n")
set.seed(int.seed[i])
folds = createFolds(label, k = 5)
for (this.round in 1:length(folds)) {
cat("model training",i," ","fold ",this.round,"\n")
valid = c(1:length(label2))[unlist(folds[this.round])]
dev = c(1:length(label2))[unlist(folds[1:length(folds)!= this.round])]
dtrain<- xgb.DMatrix(data= as.matrix(df_train[dev,]), label= label2[dev])
dvalid <- xgb.DMatrix(data= as.matrix(df_train[valid,]),label=label2[valid])
valids <- list(val = dvalid)
param = list(booster = "gbtree",
objective = "reg:linear",
eval_metric = "rmse",
eta = 0.01,
colsample_bytree = 0.8,
max_depth = 5,
min_child_weight = 1,
#num_parallel_tree = 2,
nthread = 8,
#base_score = mean(label),
gamma = 0,
subsample = 0.8
)
model<- xgb.train(data = dtrain,
params= param,
nrounds = 1000,
verbose = T,
list(val = dvalid) ,
early_stopping_rounds = 100 ,
print_every_n = 500,
maximize = F
)
pred = predict(model,as.matrix(df_train[valid,]))
devresult[valid] = pred
pred_test = predict(model, as.matrix(df_test[,colnames(df_train)]))
predte = predte + pred_test
}
}
pred =  predte/ 5
sub = data.frame(test.id,pred)
colnames(sub) = colnames(samp)
write.csv(sub,file = paste0(subm.dir,"/submit.csv"),row.names = F)
da$mimm=NULL
da$mamm=NULL
da$msdm=NULL
da$xgb = devresult
de$xgb = predte/5
summary(predte.5)
summary(predte/5)
df_train = df_train %>% select(-ends_with("RECI"))
df_train = df_train %>% select(-ends_with("RECI2"))
df_train = df_train %>% select(-ends_with("CRED"))
devresult = rep(0,nrow(df_train))
predte = rep(0,nrow(df_test))
cvscore = c()
int.seed = c(54321)##5000
for (i in 1:length(int.seed)) {
cat("model training",i,"\n")
set.seed(int.seed[i])
folds = createFolds(label, k = 5)
for (this.round in 1:length(folds)) {
cat("model training",i," ","fold ",this.round,"\n")
valid = c(1:length(label2))[unlist(folds[this.round])]
dev = c(1:length(label2))[unlist(folds[1:length(folds)!= this.round])]
dtrain<- xgb.DMatrix(data= as.matrix(df_train[dev,]), label= label2[dev])
dvalid <- xgb.DMatrix(data= as.matrix(df_train[valid,]),label=label2[valid])
valids <- list(val = dvalid)
param = list(booster = "gbtree",
objective = "reg:linear",
eval_metric = "rmse",
eta = 0.01,
colsample_bytree = 0.8,
max_depth = 6,
min_child_weight = 1,
#num_parallel_tree = 2,
nthread = 8,
#base_score = mean(label),
gamma = 0,
subsample = 0.8
)
model<- xgb.train(data = dtrain,
params= param,
nrounds = 1000,
verbose = T,
list(val = dvalid) ,
early_stopping_rounds = 100 ,
print_every_n = 500,
maximize = F
)
pred = predict(model,as.matrix(df_train[valid,]))
devresult[valid] = pred
pred_test = predict(model, as.matrix(df_test[,colnames(df_train)]))
predte = predte + pred_test
}
}
pred = predte/5
sub = data.frame(test.id,pred)
colnames(sub) = colnames(samp)
write.csv(sub,file = paste0(subm.dir,"/submit.csv"),row.names = F)
de$mimm=NULL
de$msdm=NULL
de$mamm=NULL
de$xgb2 = predte/5
da$xgb2 = devresult
dtrain = xgb.DMatrix(as.matrix(da), label=label)
dtest = xgb.DMatrix(as.matrix(de))
param = list(booster = "gbtree",
objective = "reg:squarederror",
eval_metric = "rmse",
eta = 0.1,
colsample_bytree = 0.8,
max_depth = 4,
min_child_weight = 1,
nthread = 8,
# base_score = mean(ll),
gamma = 0,
subsample = 0.8
)
watchlist= list(train = dtrain)
set.seed(1235)
fit_cv = xgb.cv(params = param,
data = dtrain,
watchlist = watchlist,
nrounds = 5000,
nfold = 5,
print_every_n = 500,
early_stopping_rounds = 100,
prediction = FALSE,
maximize = FALSE)
set.seed(1235)
mod.xgb = xgb.train(data = dtrain,params = param,nrounds = fit_cv$best_ntreelimit)
imp = as.data.frame(xgb.importance(feature_names = colnames(df_train),model = mod.xgb))
param = list(booster = "gbtree",
objective = "reg:linear",
eval_metric = "rmse",
eta = 0.1,
colsample_bytree = 0.8,
max_depth = 4,
min_child_weight = 1,
nthread = 8,
# base_score = mean(ll),
gamma = 0,
subsample = 0.8
)
watchlist= list(train = dtrain)
set.seed(1235)
fit_cv = xgb.cv(params = param,
data = dtrain,
watchlist = watchlist,
nrounds = 5000,
nfold = 5,
print_every_n = 500,
early_stopping_rounds = 100,
prediction = FALSE,
maximize = FALSE)
set.seed(1235)
mod.xgb = xgb.train(data = dtrain,params = param,nrounds = fit_cv$best_ntreelimit)
imp = as.data.frame(xgb.importance(feature_names = colnames(df_train),model = mod.xgb))
preds= predict(mod.xgb,dtest)
#pred = round(pred^2)
sub = data.frame(test.id,preds)
colnames(sub) = colnames(samp)
write.csv(sub,file = paste0(subm.dir,"/subm.csv"),row.names = F)
summary(preds)
da$xgb=NULL
da$xgb2=NULL
library(catboost)
#p = matrix(0,ncol = 7,nrow = 19766)
dtest = catboost.load_pool(as.matrix(de))
### create a matrix for predictions
p <- rep(0, nrow(df_test))
int.seed = c(500)#,1235,101,54321,2021)
set.seed(int.seed[i])
folds = createFolds(label, k = 5)
p <- rep(0, nrow(df_test))
for (i in 1) {
print(paste0("model training on label ", i))
#label = cv_label[,i]
for(rnd in 1:length(folds)){
valid = c(1:length(label2))[unlist(folds[rnd])]
dev = c(1:length(label2))[unlist(folds[1:length(folds) != rnd])]
dtrain = catboost.load_pool(as.matrix(da[dev,]), label=label2[dev])
dvalid = catboost.load_pool(as.matrix(da[valid,]),label=label2[valid])
params = list(
iterations = 10000,
learning_rate = 0.1,
depth = 3,
eval_metric = "RMSE",
loss_function = "RMSE",
random_seed = 1235,
use_best_model = TRUE,
logging_level = "Verbose",
rsm = 0.8,
od_type = "Iter",
od_wait = 100,
metric_period = 1000
)
# set.seed(1235)
model = catboost.train(learn_pool = dtrain,test_pool = dvalid,
params = params)
pred = catboost.predict(model, pool = dtest)
p = p + pred
}
}
preds = p/5
summaryRprof(preds)
summary(preds)
sub = data.frame(test.id,preds)
colnames(sub) = colnames(samp)
write.csv(sub,file = paste0(subm.dir,"/subm.csv"),row.names = F)
w = p/
w = p/5
de$xgb=NULL
de$xgb2=NULL
dtrain = xgb.DMatrix(as.matrix(da), label=label)
dtest = xgb.DMatrix(as.matrix(de))
param = list(booster = "gbtree",
objective = "reg:squarederror",
eval_metric = "rmse",
eta = 0.1,
colsample_bytree = 0.8,
max_depth = 4,
min_child_weight = 1,
nthread = 8,
# base_score = mean(ll),
gamma = 0,
subsample = 0.8
)
watchlist= list(train = dtrain)
set.seed(1235)
fit_cv = xgb.cv(params = param,
data = dtrain,
watchlist = watchlist,
nrounds = 5000,
nfold = 5,
print_every_n = 500,
early_stopping_rounds = 100,
prediction = FALSE,
maximize = FALSE)
set.seed(1235)
mod.xgb = xgb.train(data = dtrain,params = param,nrounds = fit_cv$best_ntreelimit)
imp = as.data.frame(xgb.importance(feature_names = colnames(df_train),model = mod.xgb))
preds= predict(mod.xgb,dtest)
#pred = round(pred^2)
cor(preds,w)
summary(w)
e = preds
preds = 0.7*e+0.3*w
sub = data.frame(test.id,preds)
colnames(sub) = colnames(samp)
write.csv(sub,file = paste0(subm.dir,"/subm.csv"),row.names = F)
summary(preds)
summary(e)
df_train = da
df_test = de
lgb_oof = rep(0,nrow(df_train))
predte = rep(0,nrow(df_test))
cvscore = c()
int.seed = c(500)#,1235,101,54321,2021)
label2= label
s = Sys.time()
for (i in 1:length(int.seed)) {
cat("model training",i,"\n")
set.seed(int.seed[i])
folds = createFolds(label, k = 5)
param = list(objective = "regression",
metric = "rmse",
#boost_from_average = "false",
#tree_learner = "voting",
feature_fraction = 0.8,
bagging_freq = 5,
bagging_fraction = 0.8
#lambda = 1,
#alpha = 1
# max_bin = 50
# min_data_in_leaf = 90,
#  min_sum_hessian_in_leaf = 10
#min_split_gain = 0.1,
)
for (this.round in 1:length(folds)) {
cat("model training",i," ","fold ",this.round,"\n")
valid = c(1:length(label))[unlist(folds[this.round])]
dev = c(1:length(label))[unlist(folds[1:length(folds)!= this.round])]
dtrain = lgb.Dataset(data = as.matrix(df_train[dev,]),
label = label2[dev], free_raw_data = F)
dvalid = lgb.Dataset(data = as.matrix(df_train[valid,]),
label = label2[valid],free_raw_data= F)
model = lgb.train(data = dtrain,
params = param,
nrounds = 2500,
valids = list(val = dvalid),
boosting_type = "gbdt",
learning_rate = 0.01,
max_depth = -1,
num_leaves = 25,
num_threads = 8,
eval_freq = 1000,
seed = 1235,
verbose = 1,
early_stopping_rounds = 100
)
pred = predict(model,as.matrix(df_train[valid,]))
lgb_oof[valid] = pred
pred_test = predict(model, as.matrix(df_test[,colnames(df_train)]))
predte = predte +pred_test
cat("model cv rmse score:", model$best_score,"\n")
cvscore = c(cvscore, model$best_score)
cat("model cv rmse mean score:",mean(cvscore), "\n")
}
}
Sys.time() - s
preds= predte/5
sub = data.frame(test.id,preds)
colnames(sub) = colnames(samp)
write.csv(sub,file = paste0(subm.dir,"/subm.csv"),row.names = F)
summary(preds)
rm(df_train)
rm(df_test)
df_train$m=NULL
df_train$mimm=NULL
df_train$mamm=NULL
df_train$msdm=NULL
lgb_oof = rep(0,nrow(df_train))
predte = rep(0,nrow(df_test))
cvscore = c()
int.seed = c(500)#,1235,101,54321,2021)
label2= label
s = Sys.time()
for (i in 1:length(int.seed)) {
cat("model training",i,"\n")
set.seed(int.seed[i])
folds = createFolds(label, k = 5)
param = list(objective = "regression",
metric = "rmse",
#boost_from_average = "false",
#tree_learner = "voting",
feature_fraction = 0.8,
bagging_freq = 5,
bagging_fraction = 0.8
#lambda = 1,
#alpha = 1
# max_bin = 50
# min_data_in_leaf = 90,
#  min_sum_hessian_in_leaf = 10
#min_split_gain = 0.1,
)
for (this.round in 1:length(folds)) {
cat("model training",i," ","fold ",this.round,"\n")
valid = c(1:length(label))[unlist(folds[this.round])]
dev = c(1:length(label))[unlist(folds[1:length(folds)!= this.round])]
dtrain = lgb.Dataset(data = as.matrix(df_train[dev,]),
label = label2[dev], free_raw_data = F)
dvalid = lgb.Dataset(data = as.matrix(df_train[valid,]),
label = label2[valid],free_raw_data= F)
model = lgb.train(data = dtrain,
params = param,
nrounds = 2500,
valids = list(val = dvalid),
boosting_type = "gbdt",
learning_rate = 0.01,
max_depth = -1,
num_leaves = 25,
num_threads = 8,
eval_freq = 1000,
seed = 1235,
verbose = 1,
early_stopping_rounds = 100
)
pred = predict(model,as.matrix(df_train[valid,]))
lgb_oof[valid] = pred
pred_test = predict(model, as.matrix(df_test[,colnames(df_train)]))
predte = predte +pred_test
cat("model cv rmse score:", model$best_score,"\n")
cvscore = c(cvscore, model$best_score)
cat("model cv rmse mean score:",mean(cvscore), "\n")
}
}
Sys.time() - s
l = lgb.importance(model)
View(l)
i = l$Feature[1:50]
da = cbind(da,df_train %>% select(i))
de = cbind(de,df_test %>% select(i))
#xgboost
da = da %>% select(-ends_with(c("vap","vpd","B8A","Temp","swe","pdsi","B10","B11")))
dtrain = xgb.DMatrix(as.matrix(da), label=label)
dtest = xgb.DMatrix(as.matrix(de))
param = list(booster = "gbtree",
objective = "reg:squarederror",
eval_metric = "rmse",
eta = 0.1,
colsample_bytree = 0.8,
max_depth = 4,
min_child_weight = 1,
nthread = 8,
# base_score = mean(ll),
gamma = 0,
subsample = 0.8
)
watchlist= list(train = dtrain)
set.seed(1235)
fit_cv = xgb.cv(params = param,
data = dtrain,
watchlist = watchlist,
nrounds = 5000,
nfold = 5,
print_every_n = 500,
early_stopping_rounds = 100,
prediction = FALSE,
maximize = FALSE)
set.seed(1235)
mod.xgb = xgb.train(data = dtrain,params = param,nrounds = fit_cv$best_ntreelimit)
imp = as.data.frame(xgb.importance(feature_names = colnames(df_train),model = mod.xgb))
preds= predict(mod.xgb,dtest)
#pred = round(pred^2)
sub = data.frame(test.id,preds)
colnames(sub) = colnames(samp)
write.csv(sub,file = paste0(subm.dir,"/subm.csv"),row.names = F)
dtest = xgb.DMatrix(as.matrix(de))
dtest = xgb.DMatrix(as.matrix(de[,colnames(da)]))
preds= predict(mod.xgb,dtest)
#pred = round(pred^2)
sub = data.frame(test.id,preds)
colnames(sub) = colnames(samp)
write.csv(sub,file = paste0(subm.dir,"/subm.csv"),row.names = F)
